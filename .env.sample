# Example environment configuration
# Base URL for Xinference server
XINFERENCE_URL=http://localhost:9997
# Whisper model name and size to launch
XINFERENCE_MODEL_NAME=whisper
XINFERENCE_MODEL_SIZE=small

# Ollama server base URL and model name
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
